Chalmers argues there is a significant likelihood of living within a simulation. This conclusion is reliant on two parts. The first entails the likelihood that we can simulate, the chance we will simulate, and how many civilizations we simulate. Chalmer concludes a 25% chance that we are simulated using a product of those probabilities.The second part is our supposed ability to simulate consciousness, given that we can simulate all other physical processes. He states that there is no evidence that directs us towards an inability to simulate consciousness. Chalmers thinks that a more advanced civilization could readily create models and algorithms that completely capture all aspects of reality, like light diffraction, joyous laughter, and vivid emotion. The two parts combine to show the a) probability of simulation and b) possibility of simulation. I examine a main response to Chalmers’s argument for the existence of simulated consciousness, and show how it reduces, and present why it fails to hinder the possibility of living in a simulation.
Let’s start with a clear definition of simulation. Chalmer raises the idea of a few main types of simulation. There is the pure simulation, where everything is completely simulated. This is like a little game of Civilization VI by Sid Meier, where everything is controlled by a computer program. The other option is the impure simulation (or the mixed simulation), which is more similar to the Matrix movies. In this case, you could be real person who happens to be donning a Meta Quest 3, and having some ultra realistic input/output system, or a computer character. The simulation hypothesis states that we could be living in a well-designed simulation of either type.
Chalmers’s reasoning is as follows. Firstly, we know that there already exist computer simulations that can predict and mimic “real” world events. Some instances include weather prediction, aerodynamics models, and disease propagation. As humans, even though we don’t know every detail, we can reproduce the important behaviours of all these phenomena. Secondly, it is plausible that, given sufficient time and resources, one could create a one-to-one replica of all functions. This would go beyond simple weather mappings: think more along the lines of complete behavioural prediction. These two statements are an extension of current observations and historical trends that we’ve exhibited, seeing that models become more and more accurate and advanced.
A more controversial topic to simulate would be of consciousness. Many people find it dubious that we would be able to algorithmically model our consciousness. The argument is that there is no simulation that captures consciousness. Thus, since we know we are conscious, we could not be in a simulation. Chalmers asserts otherwise. He tests the inconceivability of simulated consciousness by using the gradual replacement experiment. This procedure entails a slow replacement of the neurons in the conscious brain over time. Small pieces of the subject's brain are removed, and then a simulated brain with physically identical functions is put there instead. The subject will exclaim that they are conscious after the first replacement, and then, with each replacement after, will likely keep touting their consciousness. However, those who don’t believe in the consciousness of a simulated brain will conclude that the patient after the final operation (which completes the replacement of the physical brain with a completely simulated brain) is not conscious.
So, when does the patient become not conscious? Does consciousness flip at a threshold of n replaced neurons? Or is it a gradual decrement in consciousness? Both mechanisms have significant flaws. If it is the former, that means there is some neuron that, when flipped, means you are not conscious. Then, within that neuron, there is some molecule that, when simulated, flips the neuron, and thus flips your consciousness. So on and so forth, one can get to the subatomic level. This is bizarre, since by tying my consciousness to a particle, the removal of that particle would also remove my consciousness. If it is the latter, that means that even though half your consciousness is gone, you still report being completely conscious. This disconnect between your perceived consciousness and your actual consciousness is also implausible.
Thus, the best and final option is that you actually retain your consciousness at the end of the entire set of operations. The same level of consciousness is kept with every succeeding operation. Thus, the simulated brain is also a conscious brain. If consciousness is simulable, there remains nothing preventing a full simulation of reality.
One objection that a skeptic might raise is that the thresholded consciousness and gradual descent in consciousness are both possible alternatives. Consider the following similar experiment. Rather than replacing the neurons one by one over time, we instead turn neurons off (or remove them). I think it is reasonable to assume that completely disabling the neuron is equivalent to removing it. There may be some biological difference, so for my argument, I will remove the neurons completely. Poof. My subject, let’s call her Tina, slowly loses neurons, and at the end of the operation, is completely devoid of any neurons. The lack of neurons implies a lack of consciousness, assuming that you need a brain to be conscious.
Since I’ve reduced consciousness to “zero”, there must have either been a point in time where consciousness was flipped like a light switch, or Tina’s consciousness dwindled from normal levels to “zero”. I don’t necessarily know which one is the case, but it seems to be that either one of them is the case. Contrary to the replacement experiment, there is no third option where consciousness is retained, unless consciousness is completely independent from functioning neurons. 
I think Chalmers would accept this argument as plausible. By accepting that, it suddenly appears that there is a reasonable concern that you could indeed lose your consciousness via neuron replacement (rather than removal/disability). The obvious disanalogy is that in one case, the number of functional neurons is decreasing, whereas otherwise it remains constant. To illustrate the difference, let’s set up a small brain example: starting with n real neurons. In Chalmers’s case, n real neurons are gradually replaced with n simulated neurons, whereas in my case, n real neurons are reduced to 0 neurons.
The difference in number of neurons raises another question: Is the “amount” of consciousness that is associated with n simulated neurons actually comparable to the same number of physical neurons? In order for this to be the case, Chalmers needs a physicalist view of the problem of consciousness. This is because, in order for the simulated neurons, which are perfect physical copies, to not be “consciously” identical to the real neurons, they need to lack some non-physical information. I like to think about this as a structural question, where consciousness can stem from a physical pattern of matter if arranged properly, but you could also frame it as a functional question, where consciousness is a product of some physical function. Regardless, the conclusion is an upshot of whether or not consciousness is physical information. 
I hope it is apparent how I have reduced this argument to the familiar problem about the existence of qualia. If qualia exist, it seems that Chalmers’s argument falls apart, as the simulated neurons are more closely related to no neurons in terms of consciousness. If qualia do not exist, then Chalmers’s simulated neurons are “consciously” identical to the real neurons, and the objection falls flat.
As an interjection, I think you might notice that this seems like a rather convoluted way to raise a much simpler objection: whether or not physicalism works, since Chalmers’s argument uses it to model the brain. There is however, a variant of Chalmers’s argument, where the simulated neurons could carry the same non-physical information that the physical neurons do, in which case the argument still faces the same objection about the reduction of neuron count. In this particular instance of Chalmers’s argument, it is a reduction to the existence of qualia, but if Chalmers were able to create a simulated neuron not reliant solely on physics- then the objection is still whether or not simulated neurons are sufficiently consciously similar to the real neurons.
Returning to the objection, one common argument against the purely physical nature of consciousness is the Knowledge Argument. The Knowledge Argument entails the color experiment, where Mary, a vision scientist who knows all physical information but lives in black and white, is first exposed to color. The argument is that Mary will gain information, and it must be non-physical. I am not convinced by this argument- I think it is much more likely that Mary is discovering some already existing physical knowledge that we also did not know about. This line of reasoning uses the fact that we do not know all physical information about knowledge, so within our limited scope, it is true that color feels like non-physical information. However, it is also plausible that we could discover some physical information capturing this new experience, so I’m skeptical about this experiment.
I do not think the objection using the removal of neurons successfully defeats Chalmers’ argument. It raises a question of whether the simulated neurons have sufficient intrinsic properties to retain consciousness, or if they are as conscious as the void from the removed neurons. In Chalmers’s physical model of the brain, this is contingent on the existence of qualia. I find the Knowledge Argument insufficient to justify qualia (though there may be other arguments), especially given its reliance on an all-encompassing set of information to design the experiment. We do not have this set of information. Even if we conclude that qualia do exist, Chalmers’s argument would work given that the simulated neurons are more conscious than “zero”, and I find it plausible that if we discover non-physical information, we could properly capture it in a model. Thus, I am in concordance with Chalmers.
